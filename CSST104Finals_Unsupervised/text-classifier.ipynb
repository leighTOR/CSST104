{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gigah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id           0\n",
      "case_outcome      0\n",
      "case_title        0\n",
      "case_text       176\n",
      "dtype: int64\n",
      "  case_id case_outcome                                         case_title  \\\n",
      "0   Case1        cited  Alpine Hardwood (Aust) Pty Ltd v Hardys Pty Lt...   \n",
      "1   Case2        cited  Black v Lipovac [1998] FCA 699 ; (1998) 217 AL...   \n",
      "2   Case3        cited  Colgate Palmolive Co v Cussons Pty Ltd (1993) ...   \n",
      "3   Case4        cited  Dais Studio Pty Ltd v Bullett Creative Pty Ltd...   \n",
      "4   Case5        cited  Dr Martens Australia Pty Ltd v Figgins Holding...   \n",
      "\n",
      "                                           case_text  \n",
      "0  Ordinarily discretion exercised costs follow e...  \n",
      "1  general principles governing exercise discreti...  \n",
      "2  Ordinarily discretion exercised costs follow e...  \n",
      "3  general principles governing exercise discreti...  \n",
      "4  preceding general principles inform exercise d...  \n",
      "Training set shape: (19847,), (19847,)\n",
      "Test set shape: (4962,), (4962,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('legal_texts.csv')\n",
    "\n",
    "# Check for any missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values in 'case_text' or 'case_outcome'\n",
    "df = df.dropna(subset=['case_text', 'case_outcome'])\n",
    "\n",
    "# Define the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    tokens = [word for word in text.split() if word.lower() not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the preprocessing function to the 'case_text' column\n",
    "df['case_text'] = df['case_text'].apply(preprocess_text)\n",
    "\n",
    "# Print the first few rows to ensure preprocessing is correct\n",
    "print(df.head())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['case_text'], df['case_outcome'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of the train and test sets\n",
    "print(f'Training set shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Test set shape: {X_test.shape}, {y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.74%\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     affirmed       0.00      0.00      0.00        23\n",
      "      applied       0.25      0.01      0.01       496\n",
      "     approved       0.00      0.00      0.00        21\n",
      "        cited       0.50      0.99      0.66      2440\n",
      "   considered       0.25      0.01      0.02       353\n",
      "    discussed       0.33      0.00      0.01       206\n",
      "distinguished       0.00      0.00      0.00       110\n",
      "     followed       0.00      0.00      0.00       437\n",
      "  referred to       0.67      0.05      0.09       855\n",
      "      related       0.00      0.00      0.00        21\n",
      "\n",
      "     accuracy                           0.50      4962\n",
      "    macro avg       0.20      0.11      0.08      4962\n",
      " weighted avg       0.42      0.50      0.34      4962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gigah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\gigah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\gigah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the training data, transform the test data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict the outcomes for the test data\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming nb_classifier and tfidf_vectorizer are your trained model and vectorizer\n",
    "\n",
    "# Save the Naive Bayes model\n",
    "joblib.dump(nb_classifier, 'nb_classifier_model.pkl')\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
