{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gigah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gigah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  case_id case_outcome                                         case_title  \\\n",
      "0   Case1        cited  Alpine Hardwood (Aust) Pty Ltd v Hardys Pty Lt...   \n",
      "1   Case2        cited  Black v Lipovac [1998] FCA 699 ; (1998) 217 AL...   \n",
      "2   Case3        cited  Colgate Palmolive Co v Cussons Pty Ltd (1993) ...   \n",
      "3   Case4        cited  Dais Studio Pty Ltd v Bullett Creative Pty Ltd...   \n",
      "4   Case5        cited  Dr Martens Australia Pty Ltd v Figgins Holding...   \n",
      "\n",
      "                                           case_text  \n",
      "0  ordinarily discretion exercised cost follow ev...  \n",
      "1  general principle governing exercise discretio...  \n",
      "2  ordinarily discretion exercised cost follow ev...  \n",
      "3  general principle governing exercise discretio...  \n",
      "4  preceding general principle inform exercise di...  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Either provide a pre-trained Doc2Vec model in the \"doc2vec_model\" paramater or provide tagged documents in the \"tagged_documents\" parameter to train a new Doc2Vec model. This is a logical XOR condition.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m keywords_list \u001b[38;5;241m=\u001b[39m [[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Train Lbl2Vec model\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m lbl2vec_model \u001b[38;5;241m=\u001b[39m \u001b[43mLbl2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeywords_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeywords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m lbl2vec_model\u001b[38;5;241m.\u001b[39mfit(documents\u001b[38;5;241m=\u001b[39mdocuments, model\u001b[38;5;241m=\u001b[39mword2vec_model, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Predict labels for documents\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gigah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lbl2vec\\lbl2vec.py:146\u001b[0m, in \u001b[0;36mLbl2Vec.__init__\u001b[1;34m(self, keywords_list, tagged_documents, label_names, epochs, vector_size, min_count, window, sample, negative, workers, doc2vec_model, num_docs, similarity_threshold, similarity_threshold_offset, min_num_docs, clean_outliers, verbose)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# validate allowed tagged_documents/doc2vec_model parameter combination\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (tagged_documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m^\u001b[39m (doc2vec_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEither provide a pre-trained Doc2Vec model in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc2vec_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m paramater or provide tagged documents in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtagged_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m parameter to train a new Doc2Vec model. This is a logical XOR condition.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# validate tagged_documents\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (tagged_documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mValueError\u001b[0m: Either provide a pre-trained Doc2Vec model in the \"doc2vec_model\" paramater or provide tagged documents in the \"tagged_documents\" parameter to train a new Doc2Vec model. This is a logical XOR condition."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "import re\n",
    "from lbl2vec import Lbl2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('legal_texts.csv')\n",
    "\n",
    "# Drop rows with missing values in 'case_text' or 'case_outcome'\n",
    "df = df.dropna(subset=['case_text', 'case_outcome'])\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words and len(word) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the preprocessing function to the 'case_text' column\n",
    "df['case_text'] = df['case_text'].apply(preprocess_text)\n",
    "\n",
    "# Print the first few rows to ensure preprocessing is correct\n",
    "print(df.head())\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 5), max_features=100000)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['case_text'])\n",
    "\n",
    "# Train Word2Vec model\n",
    "documents = [text.split() for text in df['case_text']]\n",
    "word2vec_model = Word2Vec(sentences=documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Define labels (for demonstration, using case outcomes as labels)\n",
    "labels = df['case_outcome'].unique().tolist()\n",
    "\n",
    "# Create label descriptions (assuming each label has its own text description)\n",
    "label_descriptions = {label: label for label in labels}\n",
    "\n",
    "# Define keywords for each label (mock example)\n",
    "keywords_list = [[label] for label in labels]\n",
    "\n",
    "# Train Lbl2Vec model\n",
    "lbl2vec_model = Lbl2Vec(keywords_list=keywords_list)\n",
    "lbl2vec_model.fit(documents=documents, model=word2vec_model, epochs=10)\n",
    "\n",
    "# Predict labels for documents\n",
    "predicted_labels = lbl2vec_model.predict(documents)\n",
    "\n",
    "# Add the predicted labels to the dataframe\n",
    "df['predicted_label'] = predicted_labels\n",
    "\n",
    "# Calculate silhouette score to evaluate the clustering\n",
    "silhouette_avg = silhouette_score(X_tfidf, predicted_labels)\n",
    "print(f'Silhouette Score: {silhouette_avg:.2f}')\n",
    "\n",
    "# Save the model and vectorizer\n",
    "joblib.dump(lbl2vec_model, 'lbl2vec_model.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# Display the first few rows with the predicted labels\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
