{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gigah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gigah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  case_id case_outcome                                         case_title  \\\n",
      "0   Case1        cited  Alpine Hardwood (Aust) Pty Ltd v Hardys Pty Lt...   \n",
      "1   Case2        cited  Black v Lipovac [1998] FCA 699 ; (1998) 217 AL...   \n",
      "2   Case3        cited  Colgate Palmolive Co v Cussons Pty Ltd (1993) ...   \n",
      "3   Case4        cited  Dais Studio Pty Ltd v Bullett Creative Pty Ltd...   \n",
      "4   Case5        cited  Dr Martens Australia Pty Ltd v Figgins Holding...   \n",
      "\n",
      "                                           case_text  \n",
      "0  ordinarily discretion exercised cost follow ev...  \n",
      "1  general principle governing exercise discretio...  \n",
      "2  ordinarily discretion exercised cost follow ev...  \n",
      "3  general principle governing exercise discretio...  \n",
      "4  preceding general principle inform exercise di...  \n",
      "Accuracy: 60.60%\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     affirmed       0.78      0.32      0.45        22\n",
      "      applied       0.36      0.33      0.34       463\n",
      "     approved       0.25      0.12      0.17        24\n",
      "        cited       0.67      0.79      0.72      2436\n",
      "   considered       0.47      0.36      0.41       357\n",
      "    discussed       0.42      0.32      0.36       204\n",
      "distinguished       0.41      0.27      0.33       107\n",
      "     followed       0.50      0.46      0.48       468\n",
      "  referred to       0.67      0.56      0.61       864\n",
      "      related       0.83      0.29      0.43        17\n",
      "\n",
      "     accuracy                           0.61      4962\n",
      "    macro avg       0.54      0.38      0.43      4962\n",
      " weighted avg       0.59      0.61      0.59      4962\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "import re\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('legal_texts.csv')\n",
    "\n",
    "# Drop rows with missing values in 'case_text' or 'case_outcome'\n",
    "df = df.dropna(subset=['case_text', 'case_outcome'])\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words and len(word) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the preprocessing function to the 'case_text' column\n",
    "df['case_text'] = df['case_text'].apply(preprocess_text)\n",
    "\n",
    "# Print the first few rows to ensure preprocessing is correct\n",
    "print(df.head())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['case_text'], df['case_outcome'], test_size=0.2, random_state=1)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 5), max_features=100000000)\n",
    "\n",
    "# Fit and transform the training data, transform the test data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the Naive Bayes classifier with hyperparameter tuning\n",
    "nb_classifier = MultinomialNB(alpha=0.01)\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict the outcomes for the test data\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer\n",
    "joblib.dump(nb_classifier, 'nb_classifier_model.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
